{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from os.path import basename\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "#NOTE: change this according to your own data_io.py file\n",
    "import data_io\n",
    "import utils as u\n",
    "import datetime as dt\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct replications of Silver et al.\n",
    "\n",
    "\n",
    "To directly replicate the results of Silver et al. (2021), you'll need to make sure that you read in the published dataset to select articles included in their analysis. The following lines of code accomplish this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_rep = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if direct_rep:\n",
    "    all_links = pd.read_csv(f\"{data_io.DATA}article_data_cleaned.csv\", encoding = \"utf-8-sig\")\n",
    "    all_links = all_links[[\"article_date\", \"article_title\", \"article_link\", \"article_id\", \"uni\"]]\n",
    "    princeton_2019 = all_links[all_links.uni==\"princeton\"]\n",
    "    harvard_2019 = all_links[all_links.uni==\"harvard\"]\n",
    "    mit_2019 = all_links[all_links.uni==\"mit\"]\n",
    "    columbia_2019 = all_links[all_links.uni==\"columbia\"]\n",
    "    yale_2019 = all_links[all_links.uni==\"yale\"]\n",
    "else:\n",
    "    princeton_links = pd.read_csv(data_io.LINK_FNAME.replace(\"uni\", 'princeton'))\n",
    "    princeton_2019 = princeton_links[princeton_links[\"article_date\"].str.contains(\"2019\")]\n",
    "    princeton_2019 = princeton_2019.reset_index(drop = True)\n",
    "    \n",
    "    harvard_links = pd.read_csv(data_io.LINK_FNAME.replace(\"uni\", 'harvard'))\n",
    "    harvard_2019 = harvard_links[harvard_links['article_date'].str.contains('2019')]\n",
    "    harvard_2019 = harvard_2019.reset_index(drop = True)\n",
    "    \n",
    "    mit_links = pd.read_csv(data_io.LINK_FNAME.replace(\"uni\", 'mit'))\n",
    "    mit_2019 = mit_links[mit_links['article_link'].str.contains('/2019/')]\n",
    "    mit_2019 = mit_2019.sample(100)\n",
    "    mit_2019 = mit_2019.reset_index(drop = True)\n",
    "    \n",
    "    columbia_links = pd.read_csv(data_io.LINK_FNAME.replace(\"uni\", 'columbia'))\n",
    "    columbia_2019 = columbia_links[columbia_links['article_date'].str.contains('2019')]\n",
    "    columbia_2019 = columbia_2019.sample(100)\n",
    "    columbia_2019 = columbia_2019.reset_index(drop = True)\n",
    "    \n",
    "    yale_links = pd.read_csv(data_io.LINK_FNAME.replace(\"uni\", 'yale'))\n",
    "    yale_2019 = yale_links[yale_links['article_date'].str.contains('2019')]\n",
    "    yale_2019 = yale_2019.reset_index(drop = True)\n",
    "    #The first story is just the greatest hits of 2019--drop it \n",
    "    yale_2019 = yale_2019.loc[1:, :]\n",
    "    #just include stories from Yale's main campus\n",
    "    yale_2019 = yale_2019[yale_2019['article_link'].str.startswith(\"https://westcampus\")==False]\n",
    "    yale_2019 = yale_2019.reset_index(drop = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Princeton data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_princeton_data(link, article_id='TEST', fix_text = False):\n",
    "    soup = u.process_request(link)\n",
    "    article = soup.find(\"article\", {'role':'article'})\n",
    "    try:\n",
    "        title = article.find('h1').get_text().strip()\n",
    "    except:\n",
    "        title = article.find(\"h2\").get_text().strip()\n",
    "    \n",
    "    date = article.find(\"div\", \n",
    "                        class_= re.compile(\"published-date\")).get_text().strip()\n",
    "    text = article.find('div',class_=\"node__content\")\n",
    "    article_text = u.process_article_text(text.find_all(\"p\"))\n",
    "    if fix_text:\n",
    "        return article_text\n",
    "    article_container = u.setup_article_container(title = title,\n",
    "                                                 date = date,\n",
    "                                                 text = article_text,\n",
    "                                                 link = link)\n",
    "    slideshow = article.find_all(\"article\", class_=re.compile(\"media-group view-mode-slideshow\"))\n",
    "    images = []\n",
    "    captions = []\n",
    "    for s in slideshow:\n",
    "        images.extend(s.find_all(\"img\", alt=True))\n",
    "        captions.extend(s.find_all(\"figcaption\"))\n",
    "    \n",
    "    main_image = article.find(\"div\",class_=re.compile(\"news-cover-image\"))\n",
    "    if main_image:\n",
    "        if main_image.find('img', alt=True) and main_image.find('img', alt=True) not in images:\n",
    "            images.append(main_image.find(\"img\",alt=True))\n",
    "            main_caption = article.find(\"div\",class_=re.compile('cover-caption')).find(\"div\",class_=re.compile('image-caption'))\n",
    "            \n",
    "            if main_caption:\n",
    "                captions.append(main_caption)\n",
    "            else:\n",
    "                captions.append('NO CAPTION')\n",
    "            \n",
    "    other_images = article.find_all(\"article\", class_=re.compile(\"media media-image\"))\n",
    "    for o in other_images:\n",
    "        if o.find(\"img\", alt=True):\n",
    "            if o.find('img',alt=True) not in images:\n",
    "                images.append(o.find(\"img\", alt=True))\n",
    "                if o.find(\"div\", class_=re.compile(\"caption\")):\n",
    "                    captions.append(o.find(\"div\", class_=re.compile(\"caption\")))\n",
    "                elif o.find(\"figcaption\"):\n",
    "                    captions.append(o.find(\"figcaption\"))\n",
    "                else:\n",
    "                    captions.append(\"NO CAPTION\")\n",
    "    \n",
    "    if soup.find(\"iframe\"):\n",
    "        print(\"video present: \", link)\n",
    "    img_link_stem = \"https://princeton.edu\"\n",
    "    idx = 0\n",
    "    for i in range(0, len(images)):\n",
    "        article_container = u.process_image(images[i], 'princeton_',\n",
    "                                           article_container, idx,\n",
    "                                           img_stem = img_link_stem)\n",
    "        try:\n",
    "            caption = captions[i].get_text().strip().replace(\"\\xa0\", \"\")\n",
    "        except:\n",
    "            caption = 'NO CAPTION'\n",
    "        article_container.loc[idx, 'image_captions'] = caption\n",
    "        idx += 1\n",
    "    embedded = article.find_all(\"iframe\")\n",
    "    for e in embedded:\n",
    "        if e.has_attr('src'):\n",
    "            img = e['src']\n",
    "            if 'youtube' in img or 'yt' in img:\n",
    "                article_container = u.process_image(img, 'princeton_',\n",
    "                                               article_container, idx,\n",
    "                                               image_type = 'youtube')\n",
    "                idx += 1\n",
    "    article_text_fmt = u.remove_captions(article_container['image_captions'].to_list(),\n",
    "                                            article_text)\n",
    "    article_container.loc[0, 'article_text'] = article_text_fmt\n",
    "    article_container['article_id'] = article_id\n",
    "    return article_container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "princeton_data = ARTICLE_CONTAINER.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in princeton_2019.index:\n",
    "    tmp = get_princeton_data(princeton_2019.loc[i, 'article_link'],\n",
    "                                princeton_2019.loc[i, 'article_id'])\n",
    "    tmp.loc[0, 'article_date_scrape'] = princeton_2019.loc[i, 'article_date']\n",
    "    tmp.loc[0, 'article_title_scrape'] = princeton_2019.loc[i, 'article_title']\n",
    "    princeton_data = pd.concat([princeton_data, tmp], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "princeton_clean = u.clean_save_dataset(princeton_data, data_io.DATA_FNAME.replace(\"uni\", 'princeton'),\n",
    "                                    data_io.FMTD_DATA_FNAME.replace('uni', 'princeton').replace('year','2019'),\n",
    "                                      sample_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get MIT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mit_data(link, article_id='TEST', fix_text = False):\n",
    "    soup = u.process_request(link)\n",
    "    #Get article info\n",
    "    #article text\n",
    "    article_body = soup.find(\"div\", class_=\"news-article--content--body--inner\")\n",
    "    \n",
    "    article_text = u.process_article_text(article_body.find_all(\"p\"))\n",
    "    date_container = soup.find(\"div\", class_=\"news-article--publication-date\")\n",
    "    date = date_container.find(\"time\").get_text().strip()\n",
    "    if fix_text:\n",
    "        return article_text\n",
    "    title = soup.find(\"h1\").get_text().strip()\n",
    "    article_container = u.setup_article_container(title = title, text = article_text,\n",
    "                                                 date = date, link = link)\n",
    "    \n",
    "    article = soup.find(\"article\")\n",
    "    \n",
    "    image_url_stem = \"https://news.mit.edu\"\n",
    "    idx = 0\n",
    "    header = soup.find(\"div\", class_='news-article--full-width-wrapper')\n",
    "    if header:\n",
    "        top_images = header.find_all(\"div\", class_=\"news-article--media--image--file\")\n",
    "        top_captions = header.find_all(\"div\", class_=\"news-article--media--image--caption\")\n",
    "    else:\n",
    "        top_images = []\n",
    "        top_captions = []\n",
    "    i = 0\n",
    "    #Get images at the top of the page\n",
    "    for i in range(0, len(top_images)):\n",
    "        if top_images[i].find(\"img\", alt=True):\n",
    "            img = top_images[i].find('img', alt=True)\n",
    "\n",
    "            img_link = img['data-src']\n",
    "            if len(top_captions) > 0:\n",
    "                caption = top_captions[i].get_text().strip().replace(\"\\xa0\", \" \")\n",
    "                if 'Caption:\\n' in caption:\n",
    "                    caption = caption.replace(\"Caption:\\n\", \"\")\n",
    "            else:\n",
    "                caption = 'NO CAPTION'\n",
    "            article_container.loc[idx, 'image_captions'] = caption\n",
    "            if 'yt' in img_link or 'youtube' in img_link:\n",
    "                article_container = u.process_image(img_link, 'mit_', article_container, idx,\n",
    "                                                       image_type = 'youtube')\n",
    "                idx += 1\n",
    "                print('found youtube video: ', img_link)\n",
    "            else:\n",
    "                article_container = u.process_image(img, 'mit_', article_container, idx,\n",
    "                                                   img_stem = image_url_stem, src_key = 'data-src')\n",
    "                idx += 1\n",
    "        \n",
    "    #Get images in the article body--these seem to be mostly thumbnails\n",
    "    i = 0\n",
    "    article_images = article_body.find_all(\"img\", alt = True)\n",
    "    for i in range(0, len(article_images)):\n",
    "        img_link = article_images[i]['src']\n",
    "        if 'yt' in img_link or 'youtube' in img_link:\n",
    "            article_container = u.process_image(img_link, 'mit_', article_container, idx,\n",
    "                                                       image_type = 'youtube')\n",
    "            idx += 1\n",
    "            print('found youtube video: ', img_link)\n",
    "        else:\n",
    "            article_container = u.process_image(article_images[i], 'mit_', article_container, idx,\n",
    "                                                img_stem = image_url_stem, src_key = ['src','data-src'])\n",
    "            idx += 1\n",
    "    article_text_fmt = u.remove_captions(article_container['image_captions'].to_list(),\n",
    "                                            article_text)\n",
    "    article_container.loc[0, 'article_text'] = article_text_fmt\n",
    "    article_container['article_id'] = article_id\n",
    "    \n",
    "    return article_container\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_2019['scraped'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_data = ARTICLE_CONTAINER.copy()\n",
    "for i in mit_2019.index:\n",
    "    print(i)\n",
    "    tmp = get_mit_data(mit_2019.loc[i, 'article_link'],\n",
    "                      mit_2019.loc[i, 'article_id'])\n",
    "    tmp.loc[0, 'article_date_scrape'] = mit_2019.loc[i, 'article_date']\n",
    "    tmp.loc[0, 'article_link'] = mit_2019.loc[i, 'article_link']\n",
    "    tmp.loc[0, 'article_title_scrape'] = mit_2019.loc[i, 'article_title']\n",
    "    mit_2019.loc[i, 'scraped'] = True\n",
    "    mit_data = pd.concat([mit_data, tmp], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_data = clean_save_dataset(mit_data, data_io.DATA_FNAME.replace(\"uni\", 'mit'),\n",
    "                                    data_io.FMTD_DATA_FNAME.replace('uni', 'mit').replace('year','2019'),\n",
    "                                     sample_size = 100, return_type = 'subset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Yale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yale_seas_data(link, article_id='TEST', fix_text = False):\n",
    "    r = requests.get(link)\n",
    "    soup = u.process_request(link)\n",
    "    idx = 0\n",
    "    article_area = soup.find(\"article\")\n",
    "    title = article_area.find(\"h1\").get_text().strip().replace(\"\\xa0\", \" \")\n",
    "    \n",
    "    date = article_area.find(\"div\", class_=\"news-date\").get_text().strip().replace(\"\\xa0\", \" \")\n",
    "    \n",
    "    article_body = article_area.find('div',class_=\"news-body\")\n",
    "    article_text = u.process_article_text(article_body.find_all(\"p\"))\n",
    "    if fix_text:\n",
    "        return article_text\n",
    "    image_stem = \"https://seas.yale.edu\"\n",
    "    article_container = u.setup_article_container(text = article_text,\n",
    "                                                 date = date,\n",
    "                                                 title = title,\n",
    "                                                 link = link)\n",
    "    article_container.loc[0, 'url_redirect'] = r.url\n",
    "    \n",
    "    feature_img = article_area.find(\"div\", class_=\"news-image\")\n",
    "    if feature_img:\n",
    "        if feature_img.find(\"img\", alt = True):\n",
    "            img = feature_img.find(\"img\", alt=True)\n",
    "            article_container = u.process_image(img, 'yale_', article_container,\n",
    "                                                idx, img_stem = image_stem)\n",
    "            idx += 1\n",
    "        elif feature_img.find(\"iframe\"):\n",
    "            if feature_img.find(\"iframe\").has_attr(\"src\"):\n",
    "                article_container = u.process_image(feature_img.find(\"iframe\")['src'],\n",
    "                                                   'yale', article_container,\n",
    "                                                   idx, image_type = 'youtube')\n",
    "                idx += 1\n",
    "    \n",
    "    images = article_body.find_all(\"img\", alt=True)\n",
    "    for i in images:\n",
    "        article_container = u.process_image(i, 'yale_', article_container,\n",
    "                                                idx, img_stem = image_stem)\n",
    "        idx += 1\n",
    "        \n",
    "    video_objs = article_body.find_all(\"object\", class_=re.compile(\"youtube\"))\n",
    "    for v in video_objs:\n",
    "        vid_url = v['data']\n",
    "        article_container = u.process_image(vid_url,'yale', article_container,\n",
    "                                            idx, image_type = 'youtube')\n",
    "        idx += 1\n",
    "    article_text_fmt = u.remove_captions(article_container['image_captions'].to_list(),\n",
    "                                            article_text)\n",
    "    article_container.loc[0, 'article_text'] = article_text_fmt\n",
    "    article_container['article_id'] = article_id\n",
    "    return article_container\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yale_data(link, article_id = 'TEST', fix_text = False):\n",
    "    r = requests.get(link)\n",
    "    if 'yale' not in r.url:\n",
    "        article_container = u.ARTICLE_CONTAINER.copy()\n",
    "        article_container.loc[0, 'article_link'] = link\n",
    "        article_container.loc[0, 'url_redirect'] = r.url\n",
    "        article_container.loc[0, 'article_text'] = 'FAILED--OUTSIDE REDIRECT'\n",
    "        article_container['article_id'] = article_id\n",
    "        return article_container\n",
    "    \n",
    "    if r.url.startswith(\"https://seas.\"):\n",
    "        print(\"SEAS article: \", article_id)\n",
    "        article_container = get_yale_seas_data(link, article_id, fix_text = fix_text)\n",
    "        return article_container\n",
    "    \n",
    "    if r.url.startswith(\"https://news.yale\") == False:\n",
    "        article_container = u.ARTICLE_CONTAINER.copy()\n",
    "        article_container.loc[0, 'article_link'] = link\n",
    "        article_container.loc[0, 'url_redirect'] = r.url\n",
    "        article_container.loc[0, 'article_text'] = 'FAILED--REDIRECT TO NON-NEWS SITE'\n",
    "        article_container['article_id'] = article_id\n",
    "        return article_container\n",
    "    \n",
    "    soup = u.process_request(link)\n",
    "    if soup.find('p', class_=\"eyebrow\"):\n",
    "        eb = soup.find('p', class_=\"eyebrow\").get_text()\n",
    "        if 'Video' in eb:\n",
    "            article_container.loc[0, 'image_links'] = 'VIDEO'\n",
    "            article_container.loc[0, 'article_text'] = 'FAILED--VIDEO ONLY'\n",
    "            article_container['article_id'] = article_id\n",
    "            return article_container\n",
    "    \n",
    "    title = soup.find(\"h1\").get_text().strip()\n",
    "    date = soup.find(\"div\", class_=\"date\").get_text().strip()\n",
    "    text_area = soup.find(\"div\", class_=re.compile(\"story clearfix\"))\n",
    "    text = u.process_article_text(text_area.find_all(\"p\"))\n",
    "    if fix_text:\n",
    "        return text\n",
    "    article_container = u.setup_article_container(text = text, date = date, title = title,\n",
    "                                                 link = link)\n",
    "    figs = soup.find_all(\"figure\")\n",
    "    idx = 0\n",
    "    for f in figs:\n",
    "        skip = False\n",
    "        for p in f.parents:\n",
    "            if p.name == 'footer':\n",
    "                skip = True\n",
    "        if skip == False:\n",
    "            images = f.find_all(\"img\", alt=True)\n",
    "            if len(images)>0:\n",
    "                if f.find(\"figcaption\"):\n",
    "                    article_container.loc[idx, 'image_captions'] = f.find(\"figcaption\").get_text().strip()\n",
    "\n",
    "                for i in images:\n",
    "                    article_container = u.process_image(i, 'yale_', article_container,\n",
    "                                                        idx, img_stem = 'https://news.yale.edu')\n",
    "\n",
    "                    idx += 1\n",
    "            elif f.find(\"iframe\"):\n",
    "                if f.find('iframe').has_attr('src'):\n",
    "                    if 'youtube' in f.find(\"iframe\")['src'] or 'yt' in f.find('iframe')['src']:\n",
    "                        print('found video: ',fv.find(\"iframe\")['src'])\n",
    "                        article_container = u.process_image(f.find(\"iframe\")['src'], 'yale_', article_container,\n",
    "                                                           idx, image_type = 'youtube')\n",
    "                        if f.find(\"figcaption\"):\n",
    "                            article_container.loc[idx, \n",
    "                                                  'image_captions'] = f.find(\"figcaption\").get_text().strip()\n",
    "                        idx += 1\n",
    "            \n",
    "    videos = soup.find_all(\"div\", class_=re.compile(\"embedded-video\"))\n",
    "    for v in videos:\n",
    "        if v.find(\"iframe\"):\n",
    "            if v.find(\"iframe\").has_attr('src'):\n",
    "                if 'youtube' in v.find(\"iframe\")['src'] or 'yt' in v.find('iframe')['src']:\n",
    "                    print('found video: ', v.find(\"iframe\")['src'])\n",
    "                    article_container = u.process_image(v.find(\"iframe\")['src'], 'yale_', article_container,\n",
    "                                                       idx, image_type = 'youtube')\n",
    "                    idx += 1\n",
    "    article_text_fmt = u.remove_captions(article_container['image_captions'].to_list(),\n",
    "                                            article_text)\n",
    "    article_container.loc[0, 'article_text'] = article_text_fmt\n",
    "    article_container['article_id'] = article_id\n",
    "    return article_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(yale_2019)):\n",
    "    print(i)\n",
    "    tmp = get_yale_data(yale_2019.loc[i, 'article_link'],\n",
    "                       yale_2019.loc[i, 'article_id'])\n",
    "    tmp.loc[0, 'article_date_scrape'] = yale_2019.loc[i, 'article_date']\n",
    "    tmp.loc[0, 'article_link'] = yale_2019.loc[i, 'article_link']\n",
    "    tmp.loc[0, 'article_title_scrape'] = yale_2019.loc[i, 'article_title']\n",
    "    if i == 0:\n",
    "        yale_data = tmp.copy()\n",
    "    else:\n",
    "        yale_data = pd.concat([yale_data, tmp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yale_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yale_subset = u.clean_save_dataset(yale_data, data_io.DATA_FNAME.replace('uni', 'yale'),\n",
    "                                    data_io.FMTD_DATA_FNAME.replace('uni', 'yale').replace('year','2019'),\n",
    "                                     sample_size = 100, return_type='subset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Columbia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columbia_data(link, article_id='TEST', existing_titles = None,\n",
    "                     fix_text = False):\n",
    "    \n",
    "    r = requests.get(link, allow_redirects=False, headers ={'User-Agent': '...'})\n",
    "    soup = None\n",
    "    soup = BeautifulSoup(r.content, features = 'html')\n",
    "    #print(r.url)\n",
    "    article_area = soup.find(\"div\", {\"id\": 'left-area'})\n",
    "    title = article_area.find(\"h1\").get_text().strip()\n",
    "    if existing_titles or r.url != link:\n",
    "        if any(title in e for e in existing_titles):\n",
    "            article_title = 'FAILED'\n",
    "            print(\"FAILED\")\n",
    "            article_container = u.ARTICLE_CONTAINER.copy()\n",
    "            article_container.loc[0, 'article_link'] = link\n",
    "            article_container.loc[0, 'article_title'] = article_title\n",
    "            print(soup.find(\"div\", class_=re.compile(\"et_pb_section\")))\n",
    "            return article_container\n",
    "    if article_area.find(\"span\", class_=\"date\"):\n",
    "        date = article_area.find(\"span\", class_=\"date\").get_text().strip()\n",
    "    else:\n",
    "        date = article_area.find(\"time\", class_=\"datetime\").get_text().strip()\n",
    "        \n",
    "    article_text = article_area.find_all(\"p\")\n",
    "    ps = []\n",
    "    bad_classes = ['caption']\n",
    "    for p in article_text:\n",
    "        if p.has_attr(\"class\"):\n",
    "            if len([c for c in p['class'] if any(b in c for b in bad_classes)]) == 0:\n",
    "                ps.append(p)\n",
    "        else:\n",
    "            ps.append(p)\n",
    "            \n",
    "    if len(article_text) ==1:\n",
    "        print(link)\n",
    "        print(article_text)\n",
    "        \n",
    "    article_text = u.process_article_text(ps)\n",
    "    \n",
    "    if fix_text:\n",
    "        return article_text\n",
    "    article_container = u.setup_article_container(text = article_text,\n",
    "                                                 link = link,\n",
    "                                                 date = date,\n",
    "                                                 title = title)\n",
    "    \n",
    "    figs = article_area.find_all(\"div\", class_=re.compile(\"wp-caption\"))\n",
    "    idx = 0\n",
    "    if soup.find(\"iframe\"):\n",
    "        l2 = soup.find('iframe')['src']\n",
    "        if 'youtube' in l2 or 'vimeo' in l2:\n",
    "            if 'youtube' in l2:\n",
    "                article_container = u.process_image(l2, 'columbia', article_container, idx, \n",
    "                                                      src_key = 'src', img_stem = None,\n",
    "                                                     image_type = 'youtube')\n",
    "                article_container.loc[idx, 'article_id'] = article_id\n",
    "                idx += 1\n",
    "            else:\n",
    "                iframe = soup.find(\"iframe\")\n",
    "                if_soup = process_request(iframe.attrs['src'])\n",
    "                if_body = if_soup.find('body')\n",
    "                ph = if_body.find('div', class_='vp-placeholder')\n",
    "                ph = str(ph)\n",
    "                article_conainer = u.process_image(ph, 'columbia_', article_container, idx, \n",
    "                                              src_key = 'src', img_stem = '',\n",
    "                                             image_type = 'vimeo')\n",
    "                idx += 1\n",
    "        \n",
    "    else:\n",
    "        if len(figs) == 0:\n",
    "            images = article_area.find_all(\"img\", alt=True)\n",
    "            if len(images) == 0:\n",
    "                print('no figures: ', link)\n",
    "                idx += 1\n",
    "            else:\n",
    "                for image in images:\n",
    "                    if \"https://secure.gravatar\" not in image['src']:\n",
    "                        article_container = u.process_image(image, 'columbia_', article_container, idx, \n",
    "                                                      src_key = 'src')\n",
    "                        article_container.loc[idx, 'article_id'] = article_id\n",
    "                        idx += 1\n",
    "    for f in figs:\n",
    "        image = f.find(\"img\", alt=True)\n",
    "        if image:\n",
    "            image_caption = f.find(\"p\", class_=re.compile(\"caption\"))\n",
    "            if image_caption:\n",
    "                image_caption = image_caption.get_text().strip().replace(\"\\xa0\", '')\n",
    "            else:\n",
    "                image_caption = 'NO CAPTION'\n",
    "            alt = image['alt']\n",
    "            article_container = u.process_image(image, 'columbia_', article_container, idx, \n",
    "                                                      src_key = 'src')\n",
    "            article_container.loc[idx, 'image_captions'] = image_caption\n",
    "            article_container.loc[idx, 'article_id'] = article_id\n",
    "            idx += 1\n",
    "    article_text_fmt = u.remove_captions(article_container['image_captions'].to_list(),\n",
    "                                            article_text)\n",
    "    article_container.loc[0, 'article_text'] = article_text_fmt\n",
    "    article_container['article_id'] = article_id\n",
    "    return article_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_data = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(columbia_2019.index)):\n",
    "    tmp = get_columbia_data(columbia_2019.loc[i, 'article_link'],\n",
    "                           columbia_2019.loc[i, 'article_id'])\n",
    "    time.sleep(3)\n",
    "    if tmp['article_text'].dropna().duplicated(keep=False).sum() > 0:\n",
    "        print('duplicate: ', i)\n",
    "        tmp.loc[0, 'article_text'] = 'FAILED'\n",
    "        tmp.loc[0, 'article_title'] = 'FAILED'\n",
    "    cu_data = pd.concat([cu_data, tmp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_subset = cu_data.drop_duplicates(subset=['article_id'],keep='first')\n",
    "id_subset = id_subset[id_subset['article_title']!='FAILED']\n",
    "\n",
    "keep_ids = id_subset['article_id'].to_list()\n",
    "columbia_subset = cu_data[cu_data['article_id'].isin(keep_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columbia_subset = u.clean_save_dataset(columbia_subset, data_io.DATA_FNAME.replace('uni', 'columbia'),\n",
    "                                    data_io.FMTD_DATA_FNAME.replace('uni', 'columbia').replace('year','2019'),\n",
    "                                     sample_size = 100, return_type='subset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Harvard article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_harvard_data(link_to_article, article_id = 'TEST', fix_text = False):\n",
    "    soup = u.process_request(link_to_article)\n",
    "    art = soup.find_all('div', class_= \"article-content\")\n",
    "    ps = []\n",
    "    bad_classes = ['byline', 'posted-on', 'caption', 'explore']\n",
    "    for a in art:\n",
    "        tmp_ps = a.find_all(\"p\")\n",
    "        for p in tmp_ps:\n",
    "            if p.has_attr(\"class\"):\n",
    "                if len([c for c in p['class'] if any(b in c for b in bad_classes)]) == 0:\n",
    "                    ps.append(p)\n",
    "            else:\n",
    "                ps.append(p)\n",
    "    article_text = u.process_article_text(ps)\n",
    "    if fix_text:\n",
    "        return article_text\n",
    "    date = soup.find(\"time\", class_=re.compile(\"timestamp--published\")).get_text().strip()\n",
    "    title = soup.find('h1', \n",
    "                      class_=re.compile('title')).get_text()\n",
    "    \n",
    "    article_container = u.setup_article_container(title = title,\n",
    "                                                 link = link_to_article,\n",
    "                                                 date = date,\n",
    "                                                 text = article_text)\n",
    "    \n",
    "    idx = 0\n",
    "    figs = soup.find(\"main\").find(\"article\").find_all(\"figure\")\n",
    "    for f in figs:\n",
    "        images = f.find_all(\"img\", alt = True)\n",
    "        if len(images) > 0:\n",
    "            #Sometimes there are multiple images associated with the same caption\n",
    "            for img in images:\n",
    "                article_container = u.process_image(img, 'harvard_', article_container,\n",
    "                                                   idx)\n",
    "                if f.find('figcaption'):\n",
    "                    article_container.loc[idx, 'image_captions'] = f.find(\"figcaption\").get_text().strip()\n",
    "                idx += 1\n",
    "        elif f.find(\"iframe\"):\n",
    "            embed = f.find(\"iframe\")\n",
    "            if embed.has_attr(\"src\"):\n",
    "                if 'youtube' in embed['src'] or 'yt' in embed['src'] or 'vimeo' in embed['src']:\n",
    "                    print(\"thumbnail: \", link_to_article)\n",
    "                    if 'vimeo' in embed['src']:\n",
    "                        article_container = u.process_image(embed['src'], 'harvard_',\n",
    "                                                           article_container, idx,\n",
    "                                                           image_type='vimeo')\n",
    "                    else:\n",
    "                        article_container = u.process_image(embed['src'], 'harvard_',\n",
    "                                                           article_container, idx,\n",
    "                                                           image_type='youtube')\n",
    "                    if f.find(\"figcaption\"):\n",
    "                        article_container.loc[idx, 'image_captions'] = f.find(\"figcaption\").get_text().strip()\n",
    "                    idx += 1\n",
    "        elif f.find(\"video\"):\n",
    "            if f.find(\"video\").find(\"source\"):\n",
    "                #print('found mp4')\n",
    "                article_container = u.process_image(f.find(\"video\").find(\"source\"), 'harvard_',\n",
    "                                                    article_container, idx,\n",
    "                                                   src_key = ['src', 'source'])\n",
    "                if f.find('figcaption'):\n",
    "                    article_container.loc[idx, 'image_captions'] = f.find(\"figcaption\").get_text().strip()\n",
    "                idx += 1\n",
    "    article_text_fmt = u.remove_captions(article_container['image_captions'].to_list(),\n",
    "                                            article_text)\n",
    "    article_container.loc[0, 'article_text'] = article_text_fmt\n",
    "    article_container['article_id'] = article_id\n",
    "    return article_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard_data = u.ARTICLE_CONTAINER.copy()\n",
    "for i in range(0, len(harvard_2019)):\n",
    "    print(i)\n",
    "    tmp = get_harvard_data(harvard_2019.loc[i, 'article_link'],\n",
    "                           harvard_2019.loc[i, 'article_id'])\n",
    "    tmp.loc[0, 'article_date_scrape'] = harvard_2019.loc[i, 'article_date']\n",
    "    tmp.loc[0, 'article_title_scrape'] = harvard_2019.loc[i, 'article_title']\n",
    "    tmp.loc[0, 'article_link'] = harvard_2019.loc[i, 'article_link']\n",
    "    harvard_data = pd.concat([harvard_data, tmp], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_harvard_data = u.clean_save_dataset(harvard_data, data_io.DATA_FNAME.replace('uni', 'harvard'),\n",
    "                                    data_io.FMTD_DATA_FNAME.replace('uni', 'harvard').replace('year','2019'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
