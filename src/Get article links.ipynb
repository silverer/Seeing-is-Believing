{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from os.path import basename\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import data_io\n",
    "import utils as u\n",
    "import datetime as dt\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Princeton links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_princeton_links(PAGE_CONTROL=None):\n",
    "    article_df = u.ARTICLE_LINK_BASE.copy()\n",
    "    indexer = 0\n",
    "    if PAGE_CONTROL:\n",
    "        max_pages = PAGE_CONTROL\n",
    "    else:\n",
    "        max_pages = 400\n",
    "    for i in range(0, max_pages):\n",
    "        temp_url = f\"https://www.princeton.edu/news?category=971&search=&page={i}\"\n",
    "        soup = u.process_request(temp_url)\n",
    "        if type(soup) == int:\n",
    "            return article_df\n",
    "        temp = soup.find_all('div',class_='news-run item')\n",
    "        for t in temp:\n",
    "            link_text = t.find('a')['href']\n",
    "            article_df.loc[indexer, 'article_title'] = t.find('a').get_text().strip()\n",
    "            article_df.loc[indexer, 'article_link'] = 'https://www.princeton.edu'+t.find('a')['href']\n",
    "            date = re.search(\"/\\d{4}/\\d{2}/\\d{2}\", link_text).group()\n",
    "            article_df.loc[indexer, 'article_date'] = date\n",
    "            indexer += 1\n",
    "    article_df = u.process_article_link_dataset(article_df, uni = 'princeton_')\n",
    "    \n",
    "    return article_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "princeton_links = get_princeton_links(PAGE_CONTROL=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "princeton_2019 = princeton_links[princeton_links['article_date'].str.contains('2019')]\n",
    "princeton_2019 = princeton_2019.reset_index(drop = True)\n",
    "princeton_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "princeton_2019.to_csv(data_io.YEAR_LINK_FNAME.replace(\"uni\", 'princeton').replace('year', '2019'),\n",
    "                     encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get MIT links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#70,42,8\n",
    "def get_mit_links(PAGE_CONTROLS = [None, None, None]):\n",
    "    #Note: since MIT has a funky main news page, you'll need to scrape the three 3 science schools\n",
    "    mit_urls = [\"https://news.mit.edu/school/engineering?type=1\",\n",
    "                \"https://news.mit.edu/school/science?type=1\",\n",
    "                \"https://news.mit.edu/school/mit-schwarzman-college-computing?type=1\"]\n",
    "    article_df = u.ARTICLE_LINK_BASE.copy()\n",
    "    \n",
    "    indexer = 0\n",
    "    for m in range(0, len(mit_urls)):\n",
    "        if PAGE_CONTROLS[m]:\n",
    "            max_pages = PAGE_CONTROLS[m]\n",
    "        else:\n",
    "            r = requests.get(mit_urls[m])\n",
    "            soup = BeautifulSoup(r.content, features = 'html')\n",
    "            pages = soup.find('div',class_='page-term--views--header')\n",
    "            pages = pages.find(\"header\").get_text().strip()\n",
    "            page_num_text = re.search('Displaying \\d [-] \\d{1,2} of \\d{1,5} news', pages).group()\n",
    "            divisor = re.search(\"[-] \\d{1,2}\", page_num_text).group()\n",
    "            divisor = int(divisor.replace(\"- \", \"\"))\n",
    "            total_arts = re.search('of \\d{1,5}', page_num_text).group()\n",
    "            total_arts = int(total_arts.replace(\"of \", \"\"))\n",
    "            max_pages = int(math.ceil(total_arts/divisor))\n",
    "        i = 0\n",
    "        for i in range(0, max_pages+1):\n",
    "            req = f\"{mit_urls[m]}&page={i}\"\n",
    "            soup = process_request(req)\n",
    "            if type(soup) == int:\n",
    "                return article_df\n",
    "            arts = soup.find_all('article')\n",
    "            for a in arts:\n",
    "                title = a.find('h3').get_text().strip()\n",
    "                link = a['about']\n",
    "                link = 'https://news.mit.edu'+link\n",
    "                if link not in article_df['article_link'].to_list():\n",
    "                    article_df.loc[indexer, 'article_title'] = title\n",
    "                    article_df.loc[indexer, 'article_link'] = link\n",
    "                    indexer += 1\n",
    "    article_df = u.process_article_link_dataset(article_df_orig, uni = 'mit_')\n",
    "    return article_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_links = get_mit_links(PAGE_CONTROLS = [70,42,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_links.to_csv(f'{data_io.DATA}mit_links.csv', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Yale Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_yale_links(PAGE_CONTROL = None):\n",
    "    article_df = u.ARTICLE_LINK_BASE.copy()\n",
    "    indexer = 0\n",
    "    if PAGE_CONTROL:\n",
    "        max_pages = PAGE_CONTROL\n",
    "    else:\n",
    "        test_link = 'https://news.yale.edu/search?sort=created&order=desc&f%5B0%5D=field_topic%3A36'\n",
    "        #r = requests.get(test_link)\n",
    "        soup = u.process_request(test_link)\n",
    "        temp_page_count = soup.find(\"li\", class_='pager-last last').find('a')['href']\n",
    "        max_pages = re.search('page=\\d{1,4}', temp_page_count).group()\n",
    "        max_pages = max_pages.replace('page=', '')\n",
    "        max_pages = int(max_pages)\n",
    "        \n",
    "       \n",
    "    for i in range(0, max_pages):\n",
    "        soup = u.process_request(f\"https://news.yale.edu/search?sort=created&order=desc&f%5B0%5D=field_topic%3A36&page={i}\")\n",
    "        dates = soup.find_all(\"div\", class_='date')\n",
    "        links = soup.find_all('h3',class_='views-field views-field-title')\n",
    "        k = 0\n",
    "        for k in range(0, len(links)):\n",
    "            article_df.loc[indexer, 'article_title'] = links[k].find('a').get_text().strip()\n",
    "            article_df.loc[indexer, 'article_link'] = \"https://news.yale.edu\"+links[k].find('a')['href']\n",
    "            article_df.loc[indexer, 'article_date'] = dates[k].find('p').get_text().strip()\n",
    "            indexer += 1\n",
    "    article_df = u.process_article_link_dataset(article_df, uni = 'yale_')\n",
    "    return article_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yale_links = get_yale_links(PAGE_CONTROL=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yale_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yale_links.to_csv(f'{data_io.DATA}yale_links.csv', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Columbia links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columbia_links(PAGE_CONTROL = None):\n",
    "    base_link = 'https://science.fas.columbia.edu/news/'\n",
    "    if PAGE_CONTROL:\n",
    "        max_pages = PAGE_CONTROL\n",
    "    else:\n",
    "        max_pages = 101\n",
    "        \n",
    "    article_df = u.ARTICLE_LINK_BASE.copy()\n",
    "    \n",
    "    i = 1\n",
    "    indexer = 0\n",
    "    for i in range(1, max_pages):\n",
    "        soup = u.process_request(f'{base_link}page/{i}/')\n",
    "        if type(soup) == int:\n",
    "            return article_df\n",
    "        temp_links = soup.find_all('h2',class_='post-title')\n",
    "        dates = soup.find_all(\"span\", class_='mdate')\n",
    "        for i in range(0, len(temp_links)):\n",
    "            this_link = temp_links[i].find('a')['href']\n",
    "            if this_link not in article_df['article_link'].to_list():\n",
    "                article_df.loc[indexer, 'article_title'] = temp_links[i].find('a').get_text().strip()\n",
    "                article_df.loc[indexer, 'article_link'] = this_link\n",
    "                article_df.loc[indexer, 'article_date'] = dates[i].get_text()\n",
    "                indexer += 1\n",
    "    article_df = u.process_article_link_dataset(article_df, uni = 'columbia_')\n",
    "    return article_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columbia_links = get_columbia_links(PAGE_CONTROL=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columbia_links.to_csv(f'{data_io.DATA}columbia_links.csv', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Univ of Pennsylvania Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_penn_links(PAGE_CONTROL = None):\n",
    "    article_df = u.ARTICLE_LINK_BASE.copy()\n",
    "    base_url = 'https://penntoday.upenn.edu/topic/science-and-technology/'\n",
    "    \n",
    "    paginated = 'https://penntoday.upenn.edu/topic/science-and-technology?page='\n",
    "    if not PAGE_CONTROL:\n",
    "        \n",
    "        soup = u.process_request(base_url)\n",
    "        num_pages = soup.find('li', class_='pager__item pager__item--last')\n",
    "        num_pages = num_pages.find('a')['href']\n",
    "        num_pages = int(num_pages.replace('?page=', ''))\n",
    "        max_pages = num_pages\n",
    "    else:\n",
    "        max_pages = PAGE_CONTROL\n",
    "        \n",
    "    indexer = 0\n",
    "    for i in range(0, max_pages):\n",
    "        this_page = f'{paginated}{i}'\n",
    "        soup = u.process_request(this_page)\n",
    "        if type(soup) == int:\n",
    "            return article_df\n",
    "        link_container = soup.find_all(\"div\", class_=\"tease__content\")\n",
    "        #links = soup.find_all('a',class_='tease__link')\n",
    "        metadata = soup.find_all('p',class_='tease__meta')\n",
    "        for i in range(0, len(link_container)):\n",
    "            link = link_container[i].find('a',class_='tease__link')['href']\n",
    "            link = f'https://penntoday.upenn.edu{link}'\n",
    "            title = link_container[i].get_text().strip()\n",
    "            title = ' '.join(title.split())\n",
    "            title = title.strip()\n",
    "            date = metadata[i].find(\"time\").get_text()\n",
    "            article_df.loc[indexer, 'article_title'] = title\n",
    "            article_df.loc[indexer, 'article_link'] = link\n",
    "            article_df.loc[indexer, 'article_date'] = date\n",
    "            indexer += 1\n",
    "    article_df = u.process_article_link_dataset(article_df, uni = 'penn_')\n",
    "    return article_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penn_links = get_penn_links(PAGE_CONTROL=35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penn_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penn_links.to_csv(f'{data_io.DATA}penn_links.csv', encoding = 'utf-8-sig', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Harvard links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_harvard_links(PAGE_CONTROL = None):\n",
    "    #article_df = ARTICLE_LINK_BASE.copy()\n",
    "    \n",
    "    soup = u.process_request(\"https://news.harvard.edu/gazette/section/science-technology/\")\n",
    "    if type(soup) == int:\n",
    "        return article_df\n",
    "    indexer = 0\n",
    "    if PAGE_CONTROL:\n",
    "        num_pages = PAGE_CONTROL\n",
    "    else:\n",
    "        num_pages = soup.find('h3', class_=\"archive-paging__page-text\").get_text()\n",
    "        num_pages = int(num_pages.replace('Page 1 of ', ''))\n",
    "    links = []\n",
    "    dates = []\n",
    "    titles = []\n",
    "    for n in range(0, num_pages):\n",
    "        if n == 0:\n",
    "            this_link = f'https://news.harvard.edu/gazette/section/science-technology/'\n",
    "        else:\n",
    "            this_link = f'https://news.harvard.edu/gazette/section/science-technology/page/{n}/'\n",
    "        soup = u.process_request(this_link)\n",
    "        if type(soup) == int:\n",
    "            return article_df\n",
    "        \n",
    "        article_links = soup.find_all('h2', class_=\"tz-article-image__title\")\n",
    "        titles.extend([a.find(\"a\").get_text().strip() for a in article_links])\n",
    "        links.extend([a.find('a')['href'].strip() for a in article_links])\n",
    "        \n",
    "        article_dates = soup.find_all('div', class_='tz-article-image__cat-and-date')\n",
    "        dates.extend([a.find(\"time\").get_text().strip() for a in article_dates])\n",
    "    article_df = pd.DataFrame({'article_link': links,\n",
    "                              'article_date': dates,\n",
    "                              'article_title': titles})\n",
    "    article_df = u.process_article_link_dataset(article_df, uni = 'harvard_')\n",
    "    return article_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard_links = get_harvard_links(PAGE_CONTROL = 13)\n",
    "#Shuffle the dataframe\n",
    "harvard_links = harvard_links.sample(frac=1)\n",
    "harvard_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard_links.to_csv(f'{data_io.DATA}harvard_links.csv', encoding = 'utf-8-sig',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
